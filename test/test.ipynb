{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/admin-hieunn/important/VNUIS-Chatbot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin-hieunn/important/VNUIS-Chatbot/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin-hieunn/important/VNUIS-Chatbot/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2025-01-12 15:30:53,751 - INFO - Load pretrained SentenceTransformer: BAAI/bge-m3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Loading models... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin-hieunn/important/VNUIS-Chatbot/.venv/src/tts/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location, **kwargs)\n",
      "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-12 15:31:04,078] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-12 15:31:04,649] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
      "[2025-01-12 15:31:04,649] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
      "[2025-01-12 15:31:04,650] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[2025-01-12 15:31:04,650] [INFO] [logging.py:128:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "[2025-01-12 15:31:04,755] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 1024, 'intermediate_size': 4096, 'heads': 16, 'num_hidden_layers': -1, 'dtype': torch.float32, 'pre_layer_norm': True, 'norm_type': <NormType.LayerNorm: 1>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000, 'invert_mask': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/admin-hieunn/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/admin-hieunn/.cache/torch_extensions/py310_cu124/transformer_inference/build.ninja...\n",
      "/home/admin-hieunn/important/VNUIS-Chatbot/.venv/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load transformer_inference op: 0.00876760482788086 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module transformer_inference...\n",
      "2025-01-12 15:31:04,999 - INFO - Done TTS\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models Loaded!\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from io import BytesIO\n",
    "from threading import Thread\n",
    "import base64\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# Opitimized\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForSpeechSeq2Seq,\n",
    "    AutoProcessor,\n",
    "    AutoTokenizer,\n",
    "    TextIteratorStreamer,\n",
    "    pipeline,\n",
    ")\n",
    "from TTS.tts.configs.xtts_config import XttsConfig  # type: ignore\n",
    "from TTS.tts.models.xtts import Xtts  # type: ignore\n",
    "\n",
    "from routes.Xu_ly_text import Xu_ly_text_de_doc\n",
    "from vi_cleaner.vi_cleaner import ViCleaner  # type: ignore\n",
    "\n",
    "# Configure logging instead of using print statements\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "\n",
    "def chrome():\n",
    "    # Configure Chrome options\n",
    "    options = Options()\n",
    "    # options.add_argument(\"--headless=new\")  # Use the new headless mode\n",
    "    # options.add_argument(\"--disable-extensions\")\n",
    "    # options.add_argument(\"--disable-gpu\")\n",
    "    # options.add_argument(\"--no-sandbox\")\n",
    "    # options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    # options.add_argument(\"--disable-plugins\")\n",
    "    # options.add_argument(\"--disable-popup-blocking\")\n",
    "    # options.add_argument(\"--disable-translate\")\n",
    "    # options.add_argument(\"--ignore-certificate-errors\")\n",
    "    # options.add_argument(\"--allow-insecure-localhost\")\n",
    "    options.add_argument(\n",
    "        \"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    )\n",
    "\n",
    "    # # Disable image loading to speed up page loads\n",
    "    # prefs = {\n",
    "    #     \"profile.managed_default_content_settings.images\": 2,\n",
    "    #     \"profile.default_content_setting_values.stylesheets\": 2,\n",
    "    #     \"profile.default_content_setting_values.cookies\": 2,\n",
    "    #     \"profile.default_content_setting_values.javascript\": 1,  # Keep JavaScript enabled if needed\n",
    "    #     \"profile.default_content_setting_values.plugins\": 2,\n",
    "    #     \"profile.default_content_setting_values.popups\": 2,\n",
    "    #     \"profile.default_content_setting_values.geolocation\": 2,\n",
    "    #     \"profile.default_content_setting_values.notifications\": 2,\n",
    "    #     \"profile.default_content_setting_values.auto_select_certificate\": 2,\n",
    "    #     \"profile.default_content_setting_values.fullscreen\": 2,\n",
    "    #     \"profile.default_content_setting_values.mouselock\": 2,\n",
    "    #     \"profile.default_content_setting_values.mixed_script\": 2,\n",
    "    #     \"profile.default_content_setting_values.media_stream\": 2,\n",
    "    # }\n",
    "    # options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "    # # Set page load strategy to 'eager' to speed up page loads\n",
    "    # options.page_load_strategy = \"eager\"\n",
    "\n",
    "    # Specify the path to the ChromeDriver\n",
    "    service = Service(\"/usr/local/bin/chromedriver\")\n",
    "\n",
    "    # Initialize the WebDriver with the specified service and options\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    return driver\n",
    "\n",
    "driver = chrome()\n",
    "# service=Service(\"/usr/bin/chromedriver\"),\n",
    "# Initialize the WebDriver\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "\n",
    "# --------------------------Define function to load Model and Data---------------------------\n",
    "# Hàm nội bộ\n",
    "def load_embedding_model(embedding_model_path, device):\n",
    "    try:\n",
    "        embedding_model = SentenceTransformer(\n",
    "            model_name_or_path=embedding_model_path,\n",
    "            device=device,\n",
    "            model_kwargs={\"torch_dtype\": \"bfloat16\"},\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load embedding model: {e}\")\n",
    "    return embedding_model\n",
    "\n",
    "\n",
    "# def load_reranking_model(pr_model_path):\n",
    "#     pr_model = CrossEncoder(model_name=pr_model_path, device=device)\n",
    "#     return pr_model\n",
    "\n",
    "\n",
    "def load_embeddings(embeddings_path, device):\n",
    "    try:\n",
    "        text_chunks_and_embedding_df = pd.read_csv(embeddings_path)\n",
    "\n",
    "        # Convert the embedding column from JSON strings to lists of floats\n",
    "        text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\n",
    "            \"embedding\"\n",
    "        ].apply(json.loads)\n",
    "\n",
    "        # Convert embeddings to PyTorch tensors\n",
    "        embeddings = torch.tensor(\n",
    "            np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()),\n",
    "            dtype=torch.bfloat16,\n",
    "        ).to(device)\n",
    "\n",
    "        pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load embeddings: {e}\")\n",
    "\n",
    "    return embeddings, pages_and_chunks\n",
    "\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def load_model_tts(xtts_checkpoint, xtts_config, xtts_vocab):\n",
    "    clear_gpu_cache()\n",
    "    config = XttsConfig()\n",
    "    config.load_json(xtts_config)\n",
    "    XTTS_MODEL = Xtts.init_from_config(config)\n",
    "\n",
    "    use_deepspeed = torch.cuda.is_available()\n",
    "\n",
    "    XTTS_MODEL.load_checkpoint(\n",
    "        config,\n",
    "        checkpoint_path=xtts_checkpoint,\n",
    "        vocab_path=xtts_vocab,\n",
    "        use_deepspeed=use_deepspeed,\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        XTTS_MODEL.cuda()\n",
    "\n",
    "    XTTS_MODEL.eval()\n",
    "    return XTTS_MODEL\n",
    "\n",
    "\n",
    "def load_model_stt(stt_model_path: str):\n",
    "    stt_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        stt_model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_safetensors=True,\n",
    "        device_map=device,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    stt_model.eval()\n",
    "    # stt_model.to(device)\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(stt_model_path)\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=stt_model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=device,\n",
    "    )\n",
    "    return pipe, stt_model\n",
    "\n",
    "\n",
    "def load_chat_model(model_path, device):\n",
    "    try:\n",
    "        if device == \"cuda\":\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=device,\n",
    "                attn_implementation=\"flash_attention_2\",\n",
    "            )\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            # model.eval()\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.float32,\n",
    "                device_map=device,\n",
    "            )\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load language model: {e}\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# -----------------------Load model and data--------------------------------------------------\n",
    "\n",
    "print(\"Loading models... \")\n",
    "# Load model embedding\n",
    "eb_model_path = os.getenv(\"PROJECTCB1_EMBEDDING_MODEL\")\n",
    "embedding_model = load_embedding_model(\n",
    "    embedding_model_path=eb_model_path, device=device\n",
    ")\n",
    "\n",
    "# Load data\n",
    "embeddings_path = os.getenv(\"PROJECTCB1_DATA_DB\")\n",
    "embeddings, pages_and_chunks = load_embeddings(\n",
    "    embeddings_path=embeddings_path, device=device\n",
    ")\n",
    "\n",
    "# Load model TTS capleaf/viXTTS\n",
    "tts_model_path = os.getenv(\"PROJECTCB1_TTS_MODEL\")\n",
    "tts_model = load_model_tts(\n",
    "    xtts_checkpoint=f\"{tts_model_path}/model.pth\",\n",
    "    xtts_config=f\"{tts_model_path}/config.json\",\n",
    "    xtts_vocab=f\"{tts_model_path}/vocab.json\",\n",
    ")\n",
    "\n",
    "logging.info(\"Done TTS\")\n",
    "# Load reference audio for tts\n",
    "reference_audio = os.getenv(\"PROJECTCB1_REFERENCE_AUDIO\")  # Mẫu giọng nói\n",
    "\n",
    "# Load model STT openai/whisper-large-v3-turbo\n",
    "stt_model_path = os.getenv(\"PROJECTCB1_STT_MODEL\")\n",
    "pipe, stt_model = load_model_stt(stt_model_path=stt_model_path)\n",
    "\n",
    "# Load LLM\n",
    "llm_path = os.getenv(\"PROJECTCB1_LLM_MODEL\")\n",
    "model, tokenizer = load_chat_model(llm_path, device=device)\n",
    "# ------------------------------------------------------------------------------------\n",
    "rerank_model_path = os.getenv(\"PROJECTCB1_RERANK_MODEL\")\n",
    "rerank_tokenizer = AutoTokenizer.from_pretrained(rerank_model_path)\n",
    "rerank_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    rerank_model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "rerank_model.eval()\n",
    "# Load reranking\n",
    "# rr_model_path = \"embedding_model/PhoRanker\"\n",
    "# reranking_model = load_reranking_model(rr_model_path)\n",
    "\n",
    "# Dowload TTS capleaf/viXTTS\n",
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# snapshot_download(\n",
    "#     repo_id=\"capleaf/viXTTS\", repo_type=\"model\", local_dir=\"Model/TTS_model\"\n",
    "# )\n",
    "\n",
    "\n",
    "print(\"Models Loaded!\")\n",
    "\n",
    "\n",
    "# ------------------------------------------Text processing-------------------------\n",
    "def normalize_vietnamese_text(text):\n",
    "    text = Xu_ly_text_de_doc(text)\n",
    "    cleaner = ViCleaner(text)\n",
    "    text = cleaner.clean()\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def split_sentences(text, max_length=245):\n",
    "    text = (\n",
    "        text.replace(\"\\n\", \". \").replace(\";\", \".\").replace(\"?\", \".\").replace(\"!\", \".\")\n",
    "    )\n",
    "\n",
    "    sentences = re.findall(r\"[^,.]+[,.]\", text)\n",
    "    grouped_sentences = []\n",
    "    current_group = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Nếu thêm câu vào mà không vượt quá giới hạn max_length\n",
    "        if len(current_group) + len(sentence) + 1 < max_length:\n",
    "            if current_group:\n",
    "                current_group += \" \" + sentence  # Ghép câu mới vào câu trước đó\n",
    "            else:\n",
    "                current_group = sentence  # Câu đầu tiên của nhóm\n",
    "        elif len(sentence) > max_length:  # Xử lý\n",
    "            if current_group:\n",
    "                grouped_sentences.append(current_group)\n",
    "                current_group = \"\"\n",
    "            tamthoi = []\n",
    "            for i in sentence.split(\" \"):\n",
    "                tamthoi += [i]\n",
    "                if len(tamthoi) >= 40:\n",
    "                    grouped_sentences += [\" \".join(tamthoi)]\n",
    "                    tamthoi = []\n",
    "            if tamthoi:\n",
    "                grouped_sentences += [\" \".join(tamthoi)]\n",
    "        else:\n",
    "            grouped_sentences.append(current_group)  # Thêm nhóm vào list\n",
    "            current_group = sentence  # Khởi tạo nhóm mới với câu hiện tại\n",
    "\n",
    "    if current_group:\n",
    "        grouped_sentences.append(current_group)  # Thêm nhóm cuối cùng vào list\n",
    "\n",
    "    return grouped_sentences\n",
    "\n",
    "\n",
    "# processor.save_pretrained(stt_model_path)\n",
    "# model.save_pretrained(stt_model_path)\n",
    "# Hàm sử dụng cho API\n",
    "\n",
    "\n",
    "# # Retrieval function\n",
    "\n",
    "\n",
    "# Retrieval with rerank\n",
    "def retrieve_relevant_resources(\n",
    "    query: str,\n",
    "    number_result_embedding: int = 20,\n",
    "    number_result_reranking: int = 5,\n",
    "    threshold: int = -4,\n",
    "):\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    # cosine_scores = util.pytorch_cos_sim(query_embedding, embeddings)[0]\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "\n",
    "    # Get top scores with a threshold\n",
    "    # scores, indices = torch.topk(input=cosine_scores, k=n_resources_to_return)\n",
    "    scores, indices = torch.topk(input=dot_scores, k=number_result_embedding)\n",
    "    print(scores)\n",
    "\n",
    "    context_items = [pages_and_chunks[i] for i in indices]\n",
    "    results = [item[\"Relevant docs\"] for item in context_items]\n",
    "\n",
    "    pairs = [[query, result] for result in results]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = rerank_tokenizer(\n",
    "            pairs, padding=True, truncation=True, return_tensors=\"pt\", max_length=1024\n",
    "        )\n",
    "        inputs = {\n",
    "            key: value.to(\"cuda\") for key, value in inputs.items()\n",
    "        }  # Move all inputs to the same device as the model\n",
    "\n",
    "        # Compute scores\n",
    "        rerank_scores = rerank_model(**inputs, return_dict=True).logits.view(\n",
    "            -1,\n",
    "        )\n",
    "\n",
    "        top_scores, top_indices = torch.topk(rerank_scores, k=number_result_reranking)\n",
    "        # Help me add script to only take the score > -3\n",
    "        filtered_indices = top_indices[top_scores > threshold]\n",
    "        rerank_result = [results[i] for i in filtered_indices]\n",
    "\n",
    "    # return results, scores, top_scores, rerank_result\n",
    "    return rerank_result\n",
    "\n",
    "\n",
    "# Không sử dung các câu dẫn dắt, hãy trả về trực tiếp câu trả lời.\n",
    "# Đảm bảo câu trả lời giải thích rõ nhất có thể.\n",
    "# Prompt formatter\n",
    "def prompt_formatter_root(query: str, results: list) -> str:\n",
    "    context = '- \"' + '\"\\n\\n- \"'.join(results) + '\"'\n",
    "    base_prompt = \"\"\"Hãy cho bản thân không gian để suy nghĩ bằng cách trích xuất các đoạn văn có liên quan từ ngữ cảnh dưới đây trước khi trả lời câu hỏi của người dùng.\n",
    "Sử dụng các đoạn ngữ cảnh sau để trả lời câu hỏi của người dùng:\n",
    "\n",
    "{context}\n",
    "\n",
    "Câu hỏi của người dùng: \"{query}\"\n",
    "Không sử dung các câu dẫn dắt, hãy trả về trực tiếp câu trả lời. Đảm bảo câu trả lời giải thích rõ nhất có thể. \n",
    "Trả lời:\"\"\"\n",
    "    return base_prompt.format(context=context, query=query)\n",
    "\n",
    "\n",
    "# ----------------------------------Output function-----------------------------------------------\n",
    "def convert_to_wav(audio_bytes):\n",
    "    try:\n",
    "        # Sử dụng pydub để đọc tệp âm thanh từ BytesIO\n",
    "        audio = AudioSegment.from_file(BytesIO(audio_bytes))\n",
    "        # Tạo một tệp WAV trong bộ nhớ\n",
    "        wav_io = BytesIO()\n",
    "        audio.export(wav_io, format=\"wav\")\n",
    "        wav_io.seek(0)  # Đặt lại con trỏ tệp về vị trí ban đầu\n",
    "        return wav_io\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error converting audio to WAV: {e}\")\n",
    "\n",
    "\n",
    "# Hàm xử lý âm thanh và nhận dạng văn bản\n",
    "def run_stt(audio_bytes):\n",
    "    # Chuyển đổi âm thanh thành WAV\n",
    "    wav_io = convert_to_wav(audio_bytes)\n",
    "\n",
    "    # Sử dụng librosa để đọc tệp WAV và lấy dữ liệu âm thanh\n",
    "    audio, sr = librosa.load(wav_io, sr=16000)\n",
    "\n",
    "    result = pipe(\n",
    "        audio, return_timestamps=True, generate_kwargs={\"language\": \"vietnamese\"}\n",
    "    )\n",
    "    # text = Xu_ly_text(result[\"text\"])\n",
    "    return result[\"text\"]\n",
    "\n",
    "\n",
    "def run_tts(text, lang=\"vi\"):\n",
    "    if tts_model is None or not reference_audio:\n",
    "        return \"You need to run the previous step to load the model !!\", None, None\n",
    "\n",
    "    if len(text) > 100:\n",
    "        gpt_cond_len_1 = tts_model.config.gpt_cond_len\n",
    "        max_ref_length_1 = tts_model.config.max_ref_len\n",
    "        sound_norm_refs_1 = tts_model.config.sound_norm_refs\n",
    "    else:\n",
    "        gpt_cond_len_1 = 0\n",
    "        max_ref_length_1 = 1\n",
    "        sound_norm_refs_1 = True\n",
    "\n",
    "    gpt_cond_latent, speaker_embedding = tts_model.get_conditioning_latents(\n",
    "        audio_path=reference_audio,\n",
    "        gpt_cond_len=gpt_cond_len_1,\n",
    "        max_ref_length=max_ref_length_1,\n",
    "        sound_norm_refs=sound_norm_refs_1,\n",
    "        # gpt_cond_len=0,\n",
    "        # max_ref_length=1,\n",
    "        # sound_norm_refs=True,\n",
    "    )\n",
    "\n",
    "    # Chuẩn hóa\n",
    "    tts_text = normalize_vietnamese_text(text)\n",
    "    tts_texts = split_sentences(tts_text)\n",
    "\n",
    "    print(tts_texts)\n",
    "    wav_chunks = []\n",
    "    for text in tts_texts:\n",
    "        if text.strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        wav_chunk = tts_model.inference(\n",
    "            text=text,\n",
    "            language=lang,\n",
    "            gpt_cond_latent=gpt_cond_latent,\n",
    "            speaker_embedding=speaker_embedding,\n",
    "            do_sample=False,\n",
    "            length_penalty=1.0,  # 1.0\n",
    "            repetition_penalty=10.0,  # 10.0\n",
    "        )\n",
    "\n",
    "        keep_len = -1\n",
    "        wav_chunk[\"wav\"] = torch.tensor(wav_chunk[\"wav\"][:keep_len]).to(device)\n",
    "        wav_chunks.append(wav_chunk[\"wav\"])\n",
    "\n",
    "    out_wav = (\n",
    "        torch.cat(wav_chunks, dim=0).squeeze(0).cpu().numpy()\n",
    "    )  # Chuyển sang numpy array\n",
    "\n",
    "    # Chuyển đổi Tensor thành định dạng WAV\n",
    "    buffer = io.BytesIO()\n",
    "\n",
    "    # Ghi âm thanh vào buffer, đảm bảo dữ liệu đầu vào là numpy array và định dạng đúng\n",
    "    try:\n",
    "        sf.write(buffer, out_wav, 24000, format=\"WAV\")\n",
    "        buffer.seek(0)\n",
    "        wav_data = buffer.read()\n",
    "        audio_base64 = base64.b64encode(wav_data).decode(\"utf-8\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing WAV file: {e}\")\n",
    "        return None\n",
    "    # return wav_data\n",
    "    logging.info(\"Done run TTS\")\n",
    "    return audio_base64\n",
    "\n",
    "\n",
    "# ---------------------------Web searching----------------------------\n",
    "\n",
    "\n",
    "def web_searching(query: str, max_links: int = 10, max_contents: int = 2):\n",
    "    \"\"\"\n",
    "    Perform a web search and retrieve page content using a single Chrome driver instance.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "        max_links (int): Maximum number of links to fetch from search results.\n",
    "        max_contents (int): Maximum number of page contents to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        str: Combined content from the fetched web pages.\n",
    "    \"\"\"\n",
    "    results_web_searching = []\n",
    "    # driver = None\n",
    "    text_web_searching = \"\"  # Initialize the variable here\n",
    "\n",
    "    try:\n",
    "        # Initialize the Chrome driver once\n",
    "        # driver = chrome()\n",
    "        # Remove implicit wait to avoid conflicts with explicit waits\n",
    "        wait = WebDriverWait(driver, 10)  # Explicit wait\n",
    "\n",
    "        # Step 1: Fetch links from Google search results\n",
    "        search_url = f\"https://www.google.com/search?q={query}\"\n",
    "        driver.get(search_url)\n",
    "        logging.info(f\"Navigated to search URL: {search_url}\")\n",
    "\n",
    "        # Wait for search results to load using CSS selector\n",
    "        wait.until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a[jsname=\"UWckNb\"]'))\n",
    "        )\n",
    "        link_elements = driver.find_elements(By.CSS_SELECTOR, 'a[jsname=\"UWckNb\"]')\n",
    "        links = []\n",
    "\n",
    "        for link_element in link_elements[:max_links]:\n",
    "            href = link_element.get_attribute(\"href\")\n",
    "            if href:\n",
    "                links.append(href)\n",
    "                logging.info(f\"Link found: {href}\")\n",
    "\n",
    "        # Step 2: Iterate through the fetched links and get page content\n",
    "        bad_domains = [\"youtube.com\"]\n",
    "        for link in links:\n",
    "            if any(bad_domain in link for bad_domain in bad_domains):\n",
    "                logging.info(f\"Skipping unwanted domain: {link}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                driver.get(link)\n",
    "                logging.info(f\"Navigated to URL: {link}\")\n",
    "\n",
    "                # Wait for the body of the page to load\n",
    "                wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "                content = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "                if content:\n",
    "                    results_web_searching.append(content)\n",
    "                    logging.info(f\"Content fetched from {link}\")\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error fetching content from {link}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if len(results_web_searching) >= max_contents:\n",
    "                logging.info(f\"Reached maximum content limit: {max_contents}\")\n",
    "                break\n",
    "\n",
    "        # Step 3: Combine the fetched contents\n",
    "        text_web_searching = \"\\n\\n\".join(\n",
    "            [f'- \"{content}\"' for content in results_web_searching]\n",
    "        )\n",
    "        text_web_searching = text_web_searching[:20000]  # Limit to 20,000 characters\n",
    "        logging.info(\"Web searching completed successfully.\")\n",
    "\n",
    "    except Exception as general_e:\n",
    "        logging.error(f\"An unexpected error occurred: {general_e}\")\n",
    "        # Optionally, set text_web_searching to an error message or keep it as initialized\n",
    "\n",
    "    # finally:\n",
    "    #     if driver:\n",
    "    #         driver.quit()  # Ensure the driver is properly closed\n",
    "    #         logging.info(\"Chrome driver has been closed.\")\n",
    "\n",
    "    return text_web_searching\n",
    "\n",
    "\n",
    "# ---------------------------------Ask---------------------------\n",
    "\n",
    "\n",
    "def ask(query: str) -> str:\n",
    "    # time.sleep(60)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"Bạn là một trợ lí tiếng Việt hữu ích. Hãy trả lời câu hỏi của người dùng một cách chính xác.\"\"\",\n",
    "        },\n",
    "    ]\n",
    "    results = retrieve_relevant_resources(\n",
    "        query, number_result_embedding=20, number_result_reranking=3, threshold=-4\n",
    "    )\n",
    "    if len(results) == 0:\n",
    "        web_search_result = web_searching(query=query, max_contents=1)\n",
    "\n",
    "        prompt = f\"\"\"Hãy cho bản thân không gian để suy nghĩ bằng cách trích xuất các đoạn văn có liên quan từ ngữ cảnh dưới đây trước khi trả lời câu hỏi của người dùng.\n",
    "Sử dụng các đoạn ngữ cảnh sau để trả lời câu hỏi của người dùng:\n",
    "\n",
    "{web_search_result}\n",
    "\n",
    "Câu hỏi của người dùng: \"{query}\"\n",
    "Không sử dung các câu dẫn dắt, hãy trả về trực tiếp câu trả lời. Đảm bảo câu trả lời giải thích đầy đủ, rõ ràng nhất có thể. \n",
    "Trả lời:\"\"\"\n",
    "    else:\n",
    "        prompt = prompt_formatter_root(query, results)\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer, skip_prompt=True, skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    thread = Thread(\n",
    "        target=model.generate,\n",
    "        kwargs={\n",
    "            \"input_ids\": inputs[\"input_ids\"],\n",
    "            \"streamer\": streamer,\n",
    "            \"do_sample\": True,\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"temperature\": 0.01,\n",
    "            # \"top_k\": 40,\n",
    "            # \"top_p\": 0.95,\n",
    "            # \"repetition_penalty\": 1.05,\n",
    "        },\n",
    "    )\n",
    "    thread.start()  # now start the thread\n",
    "\n",
    "    # for this example we'll both print out the new text and save it to a file\n",
    "    # -----------------------------\n",
    "    try:\n",
    "        # Yield each piece of generated text as it's available\n",
    "        for new_text in streamer:\n",
    "            yield new_text + \"\"\n",
    "        # for new_text in streamer:\n",
    "        #     new_text = new_text.replace(\"\\n\", \"\\\\n\")\n",
    "        #     yield \"data: \" + new_text + \"\\n\\n\"\n",
    "    finally:\n",
    "        # Ensure the thread is properly joined even if the generator is not fully consumed\n",
    "        logging.info(\"Done ask query\")\n",
    "        thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-12 15:31:38,780 - INFO - Navigated to search URL: https://www.google.com/search?q=Hôm nay là thứ mấy\n",
      "2025-01-12 15:31:38,798 - INFO - Link found: https://lichngaytot.com/lich-am-duong.html\n",
      "2025-01-12 15:31:38,801 - INFO - Link found: https://thuvienphapluat.vn/phap-luat/am-lich-hom-nay\n",
      "2025-01-12 15:31:38,803 - INFO - Link found: https://www.tienganh123.com/tieng-anh-lop4-unit3-lecture/18236-hom-nay-la-thu-may.html\n",
      "2025-01-12 15:31:38,805 - INFO - Link found: https://soha.vn/duong-lich.html\n",
      "2025-01-12 15:31:38,808 - INFO - Link found: https://zingmp3.vn/bai-hat/Ngay-Hom-Nay-La-Ngay-Thu-May-Dang-Thinh/ZZAU7Z0W.html\n",
      "2025-01-12 15:31:38,812 - INFO - Link found: https://www.nhaccuatui.com/bai-hat/hom-nay-la-thu-may-twins.O2nY7a_UUy.html\n",
      "2025-01-12 15:31:38,817 - INFO - Link found: https://hoatieu.vn/tai-lieu/hom-nay-ngay-may-205718\n",
      "2025-01-12 15:31:38,827 - INFO - Link found: https://wordwall.net/vi/resource/31826584/h%C3%B4m-nay-l%C3%A0-th%E1%BB%A9-m%E1%BA%A5y\n",
      "2025-01-12 15:31:38,832 - INFO - Link found: https://www.xemlicham.com/\n",
      "2025-01-12 15:31:38,837 - INFO - Link found: https://loigiaihay.com/bai-tap-328739.html\n",
      "2025-01-12 15:31:39,836 - INFO - Navigated to URL: https://lichngaytot.com/lich-am-duong.html\n",
      "2025-01-12 15:31:39,994 - INFO - Content fetched from https://lichngaytot.com/lich-am-duong.html\n",
      "2025-01-12 15:31:41,535 - INFO - Navigated to URL: https://thuvienphapluat.vn/phap-luat/am-lich-hom-nay\n",
      "2025-01-12 15:31:41,596 - INFO - Content fetched from https://thuvienphapluat.vn/phap-luat/am-lich-hom-nay\n",
      "2025-01-12 15:31:41,596 - INFO - Reached maximum content limit: 2\n",
      "2025-01-12 15:31:41,596 - INFO - Web searching completed successfully.\n"
     ]
    }
   ],
   "source": [
    "a = web_searching(query= \"Hôm nay là thứ mấy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
